{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "import gc\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"./PaddleScience/\")\n",
    "sys.path.append('./3rd_lib')\n",
    "sys.path.append(\"./model\")\n",
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from timeit import default_timer\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import paddle\n",
    "import yaml\n",
    "from paddle.base import unique_name\n",
    "from paddle.optimizer.lr import LRScheduler\n",
    "from src.data import instantiate_datamodule\n",
    "from src.networks import instantiate_network\n",
    "from src.utils.average_meter import AverageMeter\n",
    "from src.utils.dot_dict import DotDict\n",
    "from src.utils.dot_dict import flatten_dict\n",
    "\n",
    "\n",
    "class StepDecay(LRScheduler):\n",
    "    def __init__(\n",
    "            self, learning_rate, step_size, gamma=0.1, last_epoch=-1, verbose=False\n",
    "    ):\n",
    "        if not isinstance(step_size, int):\n",
    "            raise TypeError(\n",
    "                \"The type of 'step_size' must be 'int', but received %s.\"\n",
    "                % type(step_size)\n",
    "            )\n",
    "        if gamma >= 1.0:\n",
    "            raise ValueError(\"gamma should be < 1.0.\")\n",
    "\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        super().__init__(learning_rate, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        i = self.last_epoch // self.step_size\n",
    "        return self.base_lr * (self.gamma ** i)\n",
    "\n",
    "\n",
    "def instantiate_scheduler(config, loader=None):\n",
    "    if config.opt_scheduler == \"CosineAnnealingLR\":\n",
    "        scheduler = paddle.optimizer.lr.CosineAnnealingDecay(\n",
    "            config.lr, T_max=config.num_epochs, verbose=True\n",
    "        )\n",
    "    elif config.opt_scheduler == \"StepLR\":\n",
    "        scheduler = StepDecay(\n",
    "            config.lr, step_size=config.opt_step_size, gamma=config.opt_gamma\n",
    "        )\n",
    "    elif config.opt_scheduler == 'OneCycleLR':\n",
    "        scheduler = paddle.optimizer.lr.OneCycleLR(\n",
    "            config.lr,\n",
    "            total_steps=(len(loader) // config.batch_size + 1) * config.num_epochs,\n",
    "            divide_factor=1000.,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Got {config.opt_scheduler=}\")\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "# loss function with rel/abs Lp loss\n",
    "class LpLoss(object):\n",
    "    def __init__(self, d=2, p=2, size_average=True, reduction=True):\n",
    "        super(LpLoss, self).__init__()\n",
    "        # Dimension and Lp-norm type are postive\n",
    "        assert d > 0 and p > 0\n",
    "\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.reduction = reduction\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def abs(self, x, y):\n",
    "        num_examples = x.size()[0]\n",
    "\n",
    "        # Assume uniform mesh\n",
    "        h = 1.0 / (x.size()[1] - 1.0)\n",
    "\n",
    "        all_norms = (h ** (self.d / self.p)) * paddle.norm(\n",
    "            x.reshape((num_examples, -1)) - y.reshape((num_examples, -1)), self.p, 1\n",
    "        )\n",
    "\n",
    "        if self.reduction:\n",
    "            if self.size_average:\n",
    "                return paddle.mean(all_norms)\n",
    "            else:\n",
    "                return paddle.sum(all_norms)\n",
    "\n",
    "        return all_norms\n",
    "\n",
    "    def rel(self, x, y):\n",
    "        diff_norms = paddle.norm(x - y, 2)\n",
    "        y_norms = paddle.norm(y, self.p)\n",
    "\n",
    "        if self.reduction:\n",
    "            if self.size_average:\n",
    "                return paddle.mean(diff_norms / y_norms)\n",
    "            else:\n",
    "                return paddle.sum(diff_norms / y_norms)\n",
    "\n",
    "        return diff_norms / y_norms\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        return self.rel(x, y)\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 0):\n",
    "    paddle.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def str2intlist(s: str) -> List[int]:\n",
    "    return [int(item.strip()) for item in s.split(\",\")]\n",
    "\n",
    "\n",
    "def parse_args(yaml=\"UnetShapeNetCar.yaml\"):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        type=str,\n",
    "        default=\"configs/\" + yaml,\n",
    "        help=\"Path to the configuration file\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--lr\", type=float, default=None, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=None, help=\"Batch size\")\n",
    "    parser.add_argument(\"--num_epochs\", type=int, default=None, help=\"Number of epochs\")\n",
    "    parser.add_argument(\n",
    "        \"--checkpoint\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Path to the checkpoint file to resume training\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\",\n",
    "        type=str,\n",
    "        default=\"./output\",\n",
    "        help=\"Path to the output directory\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log\",\n",
    "        type=str,\n",
    "        default=\"log\",\n",
    "        help=\"Path to the log directory\",\n",
    "    )\n",
    "    parser.add_argument(\"--logger_types\", type=str, nargs=\"+\", default=None)\n",
    "    parser.add_argument(\"--seed\", type=int, default=0, help=\"Random seed for training\")\n",
    "    parser.add_argument(\"--model\", type=str, default=None, help=\"Model name\")\n",
    "    parser.add_argument(\n",
    "        \"--sdf_spatial_resolution\",\n",
    "        type=str2intlist,\n",
    "        default=None,\n",
    "        help=\"SDF spatial resolution. Use comma to separate the values e.g. 32,32,32.\",\n",
    "    )\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def load_config(config_path):\n",
    "    def include_constructor(loader, node):\n",
    "        # Get the path of the current YAML file\n",
    "        current_file_path = loader.name\n",
    "\n",
    "        # Get the folder containing the current YAML file\n",
    "        base_folder = os.path.dirname(current_file_path)\n",
    "\n",
    "        # Get the included file path, relative to the current file\n",
    "        included_file = os.path.join(base_folder, loader.construct_scalar(node))\n",
    "\n",
    "        # Read and parse the included file\n",
    "        with open(included_file, \"r\") as file:\n",
    "            return yaml.load(file, Loader=yaml.Loader)\n",
    "\n",
    "    # Register the custom constructor for !include\n",
    "    yaml.Loader.add_constructor(\"!include\", include_constructor)\n",
    "\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "    # Convert to dot dict\n",
    "    config_flat = flatten_dict(config)\n",
    "    config_flat = DotDict(config_flat)\n",
    "    return config_flat\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_numbers(s):\n",
    "    return [int(digit) for digit in re.findall(r'\\d+', s)]\n",
    "\n",
    "\n",
    "def write_to_vtk(out_dict, point_data_pos=\"press on mesh points\", mesh_path=None, track=None):\n",
    "    import meshio\n",
    "    p = out_dict[\"pressure\"]\n",
    "    index = extract_numbers(mesh_path.name)[0]\n",
    "\n",
    "    if track == \"Dataset_1\":\n",
    "        index = str(index).zfill(3)\n",
    "    elif track == \"Track_B\":\n",
    "        index = str(index).zfill(4)\n",
    "\n",
    "    print(f\"Pressure shape for mesh {index} = {p.shape}\")\n",
    "\n",
    "    if point_data_pos == \"press on mesh points\":\n",
    "        mesh = meshio.read(mesh_path)\n",
    "        mesh.point_data[\"p\"] = p.numpy()\n",
    "        if \"pred wss_x\" in out_dict:\n",
    "            wss_x = out_dict[\"pred wss_x\"]\n",
    "            mesh.point_data[\"wss_x\"] = wss_x.numpy()\n",
    "    elif point_data_pos == \"press on mesh cells\":\n",
    "        points = np.load(mesh_path.parent / f\"centroid_{index}.npy\")\n",
    "        npoint = points.shape[0]\n",
    "        mesh = meshio.Mesh(\n",
    "            points=points, cells=[(\"vertex\", np.arange(npoint).reshape(npoint, 1))]\n",
    "        )\n",
    "        mesh.point_data = {\"p\": p.numpy()}\n",
    "\n",
    "    print(f\"write : ./output/{mesh_path.parent.name}_{index}.vtk\")\n",
    "    mesh.write(f\"./output/{mesh_path.parent.name}_{index}.vtk\")\n",
    "\n",
    "\n",
    "@paddle.no_grad()\n",
    "def eval(model, datamodule, config, loss_fn=None, track=\"Dataset_1\"):\n",
    "    test_loader = datamodule.test_dataloader(batch_size=config.eval_batch_size, shuffle=False, num_workers=0)\n",
    "    data_list = []\n",
    "    cd_list = []\n",
    "\n",
    "    for i, data_dict in enumerate(test_loader):\n",
    "        out_dict = model.eval_dict(data_dict, loss_fn=loss_fn, decode_fn=datamodule.decode)\n",
    "        if 'l2 eval loss' in out_dict:\n",
    "            if i == 0:\n",
    "                data_list.append(['id', 'l2 p'])\n",
    "            else:\n",
    "                data_list.append([i, float(out_dict['l2 eval loss'])])\n",
    "\n",
    "        # TODO : you may write velocity into vtk, and analysis in your report\n",
    "        if config.write_to_vtk is True:\n",
    "            print(\"datamodule.test_mesh_paths = \", datamodule.test_mesh_paths[i])\n",
    "            write_to_vtk(out_dict, config.point_data_pos, datamodule.test_mesh_paths[i], track)\n",
    "\n",
    "        # Your submit your npy to leaderboard here\n",
    "        if \"pressure\" in out_dict:\n",
    "            p = out_dict[\"pressure\"].reshape((-1,)).astype(np.float32)\n",
    "            test_indice = datamodule.test_indices[i]\n",
    "            npy_leaderboard = f\"./output/{track}/press_{str(test_indice).zfill(3)}.npy\"\n",
    "            print(f\"saving *.npy file for [{track}] leaderboard : \", npy_leaderboard)\n",
    "            np.save(npy_leaderboard, p)\n",
    "        if \"velocity\" in out_dict:\n",
    "            v = out_dict[\"velocity\"].reshape((-1, 3)).astype(np.float32)\n",
    "            test_indice = datamodule.test_indices[i]\n",
    "            npy_leaderboard = f\"./output/{track}/vel_{str(test_indice).zfill(3)}.npy\"\n",
    "            print(f\"saving *.npy file for [{track}] leaderboard : \", npy_leaderboard)\n",
    "            np.save(npy_leaderboard, v)\n",
    "        if \"cd\" in out_dict:\n",
    "            v = out_dict[\"cd\"].item()\n",
    "            test_indice = datamodule.test_indices[i]\n",
    "            cd_list.append([i, v])\n",
    "\n",
    "        # check csv in ./output\n",
    "        with open(f\"./output/{config.project_name}.csv\", \"w\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(data_list)\n",
    "\n",
    "    if \"cd\" in out_dict:\n",
    "        titles = [\"\", \"Cd\"]\n",
    "        df = pd.DataFrame(cd_list, columns=titles)\n",
    "        df.to_csv(f'./output/{track}/Answer.csv', index=False)\n",
    "    return\n",
    "\n",
    "\n",
    "def train(config):\n",
    "    paddle.device.set_device(config.device)\n",
    "    model = instantiate_network(config)\n",
    "    # checkpoint = paddle.load(f\"./output/model-{config.model}-{config.track}-{49}.pdparams\")\n",
    "    # model.load_dict(checkpoint)\n",
    "    datamodule = instantiate_datamodule(config)\n",
    "    train_loader = datamodule.train_dataloader(batch_size=config.batch_size, shuffle=False)\n",
    "    eval_dict = None\n",
    "    # Initialize the optimizer\n",
    "    scheduler = instantiate_scheduler(config, train_loader)\n",
    "    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n",
    "    optimizer = paddle.optimizer.AdamW(\n",
    "        parameters=model.parameters(), learning_rate=scheduler, weight_decay=1e-4,\n",
    "        grad_clip=clip,\n",
    "    )\n",
    "    # Initialize the loss function\n",
    "    loss_fn = LpLoss(size_average=True)\n",
    "    L2 = []\n",
    "    for ep in range(config.num_epochs):\n",
    "        model.train()\n",
    "        t1 = default_timer()\n",
    "        train_l2_meter = AverageMeter()\n",
    "        for i, data_dict in enumerate(train_loader):\n",
    "            optimizer.clear_grad(set_to_zero=False)\n",
    "            loss_dict = model.loss_dict(data_dict, loss_fn=loss_fn)\n",
    "            loss = 0\n",
    "            for k, v in loss_dict.items():\n",
    "                loss = loss + v.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_l2_meter.update(loss.item())\n",
    "\n",
    "        scheduler.step()\n",
    "        t2 = default_timer()\n",
    "        print(\n",
    "            f\"Training epoch {ep} took {t2 - t1:.2f} seconds. L2 loss: {train_l2_meter.avg:.4f}\"\n",
    "        )\n",
    "\n",
    "        L2.append(train_l2_meter.avg)\n",
    "        if ep % config.eval_interval == 0 or ep == config.num_epochs - 1:\n",
    "            # if you want to eval in Cars during training process, use data in Training datasets\n",
    "            # eval_dict = eval(model, datamodule, config, loss_fn)\n",
    "            if eval_dict is not None:\n",
    "                for k, v in eval_dict.items():\n",
    "                    print(f\"Epoch: {ep} {k}: {v.item():.4f}\")\n",
    "        # Save the weights\n",
    "        if ep % config.save_interval == 0 or ep == config.num_epochs - 1 and ep > 1:\n",
    "            paddle.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(\"./output/\", f\"model-{config.model}-{config.track}-{ep}.pdparams\"),\n",
    "            )\n",
    "    del model\n",
    "    gc.collect()\n",
    "    unique_name.dygraph_parameter_name_checker._name_set.clear()\n",
    "\n",
    "\n",
    "def load_yaml(file_name):\n",
    "    args = parse_args(file_name)\n",
    "    # args = parse_args(\"Unet_Velocity.yaml\")\n",
    "    config = load_config(args.config)\n",
    "\n",
    "    # Update config with command line arguments\n",
    "    for key, value in vars(args).items():\n",
    "        if key != \"config\" and value is not None:\n",
    "            config[key] = value\n",
    "\n",
    "    # pretty print the config\n",
    "    if paddle.distributed.get_rank() == 0:\n",
    "        print(f\"\\n--------------- Config [{file_name}] Table----------------\")\n",
    "        for key, value in config.items():\n",
    "            print(\"Key: {:<30} Val: {}\".format(key, value))\n",
    "        print(\"--------------- Config yaml Table----------------\\n\")\n",
    "    return config\n",
    "\n",
    "\n",
    "def leader_board(config, track):\n",
    "    os.makedirs(f\"./output/{track}/\", exist_ok=True)\n",
    "    model = instantiate_network(config)\n",
    "    # checkpoint = paddle.load(f\"./output/model-{config.model}-{config.track}-{config.num_epochs - 1}.pdparams\")\n",
    "    # model.load_dict(checkpoint)\n",
    "    print(f\"\\n-------Starting Evaluation over [{config.track}] --------\")\n",
    "    config.n_train = 1\n",
    "    t1 = default_timer()\n",
    "\n",
    "    config.mode = \"test\"\n",
    "    eval(\n",
    "        model, instantiate_datamodule(config), config, loss_fn=lambda x, y: 0, track=track\n",
    "    )\n",
    "    del model\n",
    "    gc.collect()\n",
    "    unique_name.dygraph_parameter_name_checker._name_set.clear()\n",
    "    t2 = default_timer()\n",
    "    print(f\"Inference over [Dataset_1 pressure] took {t2 - t1:.2f} seconds.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # try:\n",
    "    os.makedirs(\"./output/\", exist_ok=True)\n",
    "    # print(23681921/(4 * 1024 * 1024), ())\n",
    "    config_p = load_yaml(\"UnetShapeNetCar.yaml\")\n",
    "    config_p.n_train = 500\n",
    "    config_p.n_test = 50\n",
    "    print(config_p.n_test)\n",
    "    # train(config_p)\n",
    "\n",
    "    # index_list = np.loadtxt(\"./data/train_data_1_velocity/watertight_meshes.txt\", dtype=int)\n",
    "    # config_v = load_yaml(\"Unet_Velocity.yaml\")\n",
    "    # config_v.train_index_list = index_list[:500].tolist()\n",
    "    # config_v.test_index_list = index_list[500:550].tolist()\n",
    "    # train(config_v)\n",
    "    #\n",
    "    # config_cd = load_yaml(\"Unet_Cd.yaml\")\n",
    "    # index_list = np.loadtxt(\"./data/Training/Dataset_2/Label_File/dataset2_train_label.csv\", delimiter=\",\",\n",
    "    #                         dtype=str, encoding='utf-8')[:, 1][1:]\n",
    "    # print(len(index_list))\n",
    "    # config_cd.train_index_list = index_list[:500].tolist()\n",
    "    # config_cd.test_index_list = index_list[500:550].tolist()\n",
    "    # # train(config_cd)\n",
    "\n",
    "    # # test on leader_board, or do evaluation by yourself\n",
    "    leader_board(config_p, \"Gen_Answer\")\n",
    "    # leader_board(config_v, \"Gen_Answer\")\n",
    "    # leader_board(config_cd, \"Gen_Answer\")\n",
    "    # os.system(f\"zip -r -j ./output/Gen_Answer.zip ./output/Gen_Answer\")\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     exit()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!wget --header=\"Host: drive.usercontent.google.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Cookie: __Secure-ENID=12.SE=Yd0Bj-CLJ14fnd4qzdJHmwUs4B5zz46UaPC1cPJigNqqFV9PtM2CYyBpSbCkOyzUwzlEdZ1nZFf-igtGi7wSdJ_gqQSfQfh84r9egqFQAy9-GKayCRbdQKdera-2mkpuIT-c64CyR9vfNojM3hxZ9Dej-dGvtxlGjal9ttEHybw; __gsas=ID=ae0421b9a34b478c:T=1710758437:RT=1710758437:S=ALNI_MZP13R9ZOHbCzC0rgHSMrGXj6GCsg; SEARCH_SAMESITE=CgQI35oB; SID=g.a000lgiBaVVDJxM_nyJHv8SGKdW8l-Ea7eoYBmsE1puBGqeYmuoUD3CsQuje5kFlDuyAjS6tbwACgYKAewSARQSFQHGX2MiQwM3sBqt9QySR0B5yipflRoVAUF8yKo6yV91BccjS1Z6o32Ux6ew0076; __Secure-1PSID=g.a000lgiBaVVDJxM_nyJHv8SGKdW8l-Ea7eoYBmsE1puBGqeYmuoUdkz-2WaNotDz3XhWHdZaewACgYKAdMSARQSFQHGX2Mi2DyEcyhQnLt_3xtpRFnETBoVAUF8yKoObySYGWwftPorBYI_hdIh0076; __Secure-3PSID=g.a000lgiBaVVDJxM_nyJHv8SGKdW8l-Ea7eoYBmsE1puBGqeYmuoUMoY9B1xAja7hD33yfC_l8gACgYKAXESARQSFQHGX2Miy7Eh3rWIzqh1GS_d-315mhoVAUF8yKpBtafXePsFsQwbeu_1xhEZ0076; HSID=AqK8DRIVZrNtHDQEM; SSID=AF8GbXclnoQKbndEK; APISID=eaYPuocswpo7W9-H/AOCjbMPSZJ096e5h4; SAPISID=xTIl5TE60VtPYf3j/A-54gZvfpsOMhgwjL; __Secure-1PAPISID=xTIl5TE60VtPYf3j/A-54gZvfpsOMhgwjL; __Secure-3PAPISID=xTIl5TE60VtPYf3j/A-54gZvfpsOMhgwjL; NID=515=plru-gK0AjVf1Sy8SC5LvpAocQ8efRsdx0CZ1kdvR10iJMaeT-fsU6T5AyKgdFpWE-kImnEI5utU-487cxMnVkrz0_-aB-3blUbOI1wu31UI-2dAjxd2LWGEgubs2zJi8cjE8hfCzP-TgVqkwzSNkE6riAOmKvZvkdSWWgnwFiQLxr3X7et1TNvcPZvpEHzG8iqoylaY55lZje6L4CmiT0b8N2PzagLdpOZdUGkWyRJDlTsAeSnlvMM5M8ZTWjZm7UvVNqMRBh-CA4KJ-BkCavo-Pt0F4-3-qSceoZ3nNz6AUfFk_2J9dWLmijh23NbSZsQP4z2u7O2WDMwTmntI1Qc5QiIuXjxGDLDz6GQPSEMIyWZCRTsNxZtmNlKQROEsPwKKMmmylxBjRXLFtSKyLitb6oDPhkp8YbUh984lDVAuZkAOcjAuCgYFdAEmNyGGrWWFPmdaUAUZ-uxIL8lClSnbNx0zECu88QhO-QtallWvKzDYr3ZK7czaeJi2nQX53eUWfJM0eyOeeric1Ltf1wA1_w0zXPaBF5rPBONe7Rzm5phafNrSAl9XpofCiBT_B4mmb81hfFlGrBI3_4Rtj7UohFGwMaKcrIyPLBok4nQogBBPAzhPsnNFLJgBOUqQLAMDIoup982yf7v8ISeApMkZdgQ; __Secure-1PSIDTS=sidts-CjEB4E2dkZiFPZveD2L67T0j-mwN6ErulV1FpkkkhAlfHtcpBwyaLEFAl9KPDrhmId99EAA; __Secure-3PSIDTS=sidts-CjEB4E2dkZiFPZveD2L67T0j-mwN6ErulV1FpkkkhAlfHtcpBwyaLEFAl9KPDrhmId99EAA; SIDCC=AKEyXzV9L2DyLa7cOj7QEH5jq731oO3bjUfxB9xunhlp0aLuj_Yrks1puUhcLsZ8rELAfcHJjDmdqg; __Secure-1PSIDCC=AKEyXzUqTJxy1COupBctW9fNufvYQ4ZxCVzD4btsqLhg-i8UKiKC1jI3mI_5nliA_a0z-FM7PD0I; __Secure-3PSIDCC=AKEyXzU45-0-sVl11D0Mg7O6UieJpansLMEGJeXrmWGqtoNEtTeyHkgewKyRGMZVmJHm49Bzj1IM\" --header=\"Connection: keep-alive\" \"https://drive.usercontent.google.com/download?id=17M0nVsBdeiZxMnFVhKMuq89APc9pbA91&export=download&authuser=0&confirm=t&uuid=de518810-0b86-44da-9924-b3044aedae65&at=APZUnTWx_R5StqP3iGT6U7FBhIm-:1720979898499\" -c -O 'CIKM_Test.zip'"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# 74 s\n",
    "!wget --header=\"Host: drive.usercontent.google.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Cookie: __Secure-ENID=12.SE=Yd0Bj-CLJ14fnd4qzdJHmwUs4B5zz46UaPC1cPJigNqqFV9PtM2CYyBpSbCkOyzUwzlEdZ1nZFf-igtGi7wSdJ_gqQSfQfh84r9egqFQAy9-GKayCRbdQKdera-2mkpuIT-c64CyR9vfNojM3hxZ9Dej-dGvtxlGjal9ttEHybw; __gsas=ID=ae0421b9a34b478c:T=1710758437:RT=1710758437:S=ALNI_MZP13R9ZOHbCzC0rgHSMrGXj6GCsg; SEARCH_SAMESITE=CgQI35oB; SID=g.a000lgiBaVVDJxM_nyJHv8SGKdW8l-Ea7eoYBmsE1puBGqeYmuoUD3CsQuje5kFlDuyAjS6tbwACgYKAewSARQSFQHGX2MiQwM3sBqt9QySR0B5yipflRoVAUF8yKo6yV91BccjS1Z6o32Ux6ew0076; __Secure-1PSID=g.a000lgiBaVVDJxM_nyJHv8SGKdW8l-Ea7eoYBmsE1puBGqeYmuoUdkz-2WaNotDz3XhWHdZaewACgYKAdMSARQSFQHGX2Mi2DyEcyhQnLt_3xtpRFnETBoVAUF8yKoObySYGWwftPorBYI_hdIh0076; __Secure-3PSID=g.a000lgiBaVVDJxM_nyJHv8SGKdW8l-Ea7eoYBmsE1puBGqeYmuoUMoY9B1xAja7hD33yfC_l8gACgYKAXESARQSFQHGX2Miy7Eh3rWIzqh1GS_d-315mhoVAUF8yKpBtafXePsFsQwbeu_1xhEZ0076; HSID=AqK8DRIVZrNtHDQEM; SSID=AF8GbXclnoQKbndEK; APISID=eaYPuocswpo7W9-H/AOCjbMPSZJ096e5h4; SAPISID=xTIl5TE60VtPYf3j/A-54gZvfpsOMhgwjL; __Secure-1PAPISID=xTIl5TE60VtPYf3j/A-54gZvfpsOMhgwjL; __Secure-3PAPISID=xTIl5TE60VtPYf3j/A-54gZvfpsOMhgwjL; NID=515=plru-gK0AjVf1Sy8SC5LvpAocQ8efRsdx0CZ1kdvR10iJMaeT-fsU6T5AyKgdFpWE-kImnEI5utU-487cxMnVkrz0_-aB-3blUbOI1wu31UI-2dAjxd2LWGEgubs2zJi8cjE8hfCzP-TgVqkwzSNkE6riAOmKvZvkdSWWgnwFiQLxr3X7et1TNvcPZvpEHzG8iqoylaY55lZje6L4CmiT0b8N2PzagLdpOZdUGkWyRJDlTsAeSnlvMM5M8ZTWjZm7UvVNqMRBh-CA4KJ-BkCavo-Pt0F4-3-qSceoZ3nNz6AUfFk_2J9dWLmijh23NbSZsQP4z2u7O2WDMwTmntI1Qc5QiIuXjxGDLDz6GQPSEMIyWZCRTsNxZtmNlKQROEsPwKKMmmylxBjRXLFtSKyLitb6oDPhkp8YbUh984lDVAuZkAOcjAuCgYFdAEmNyGGrWWFPmdaUAUZ-uxIL8lClSnbNx0zECu88QhO-QtallWvKzDYr3ZK7czaeJi2nQX53eUWfJM0eyOeeric1Ltf1wA1_w0zXPaBF5rPBONe7Rzm5phafNrSAl9XpofCiBT_B4mmb81hfFlGrBI3_4Rtj7UohFGwMaKcrIyPLBok4nQogBBPAzhPsnNFLJgBOUqQLAMDIoup982yf7v8ISeApMkZdgQ; __Secure-1PSIDTS=sidts-CjEB4E2dkZiFPZveD2L67T0j-mwN6ErulV1FpkkkhAlfHtcpBwyaLEFAl9KPDrhmId99EAA; __Secure-3PSIDTS=sidts-CjEB4E2dkZiFPZveD2L67T0j-mwN6ErulV1FpkkkhAlfHtcpBwyaLEFAl9KPDrhmId99EAA; SIDCC=AKEyXzURbTd6qCpA02QoGXCORelVic8I05vY0Fudj1oUukq6szKRAuO4mcverQ_TFNKg0pPsF-tiZg; __Secure-1PSIDCC=AKEyXzVPd347dt-7_3JSjQc3gmzPqwRtmRgaQpuVqN0qtN34VORpmjxOBVKFZsXxU0m0JibWfONm; __Secure-3PSIDCC=AKEyXzWa_JoS5Duud9ECYjQhLb8ENX9_ML6X5YK4El1eSaET9Cq21vXwSW53TCFtQ0e-KwYt3BEK\" --header=\"Connection: keep-alive\" \"https://drive.usercontent.google.com/download?id=1Y0ejShdcx6YiaE3jJ0yxcOTKxirR-MAh&export=download&authuser=0&confirm=t&uuid=90e67168-c423-4396-a7ac-4cfcb1f2cc01&at=APZUnTXTP3YsMMcYVNsvI5-B_UtN:1720979862404\" -c -O 'CIKM_Training.zip'"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DataSet 1\n",
    "# Pressure\n",
    "!unzip -o data/data258885/train_data.zip -d /home/aistudio/data\n",
    "!unzip -o /home/aistudio/data/data258885/track_A.zip -d /home/aistudio/data/\n",
    "!mv /home/aistudio/data/track_A /home/aistudio/data/test_data_1\n",
    "!mkdir /home/aistudio/data/validate_data_1/\n",
    "!cp /home/aistudio/data/data/press_001.npy  /home/aistudio/data/test_data_1/press_658.npy\n",
    "!unzip /home/aistudio/data/data281827/train_velocity.zip -d /home/aistudio/data\n",
    "!mv /home/aistudio/data/train /home/aistudio/data/train_data_1_velocity\n",
    "!mkdir -p /home/aistudio/data/Test/Dataset_1/\n",
    "!mkdir -p /home/aistudio/data/Test/Dataset_1/Feature_File\n",
    "!unzip /home/aistudio/data/data281827/test_dataset1_velocity.zip -d /home/aistudio/data/Test/Dataset_1/Feature_File\n",
    "\n",
    "# Dataset 2 (Building)\n",
    "# geometry input: https://www.dropbox.com/scl/fo/h7q1msrlq6jdkyxxwki1s/h?dl=0&e=2\n",
    "!mkdir -p /home/aistudio/data/Training/Dataset_2/\n",
    "!mkdir -p /home/aistudio/data/Training/Dataset_2/Label_File/\n",
    "!cp /home/aistudio/data/data281827/dataset2_train_label.csv /home/aistudio/data/Training/Dataset_2/Label_File/\n",
    "!unzip /home/aistudio/data/data281827/Dataset_2_Feature.zip -d /home/aistudio/data/Training/Dataset_2/\n",
    "\n",
    "!mkdir -p /home/aistudio/data/Test/Dataset_2/Feature_File/\n",
    "!unzip /home/aistudio/data/data281827/test_data2_cd.zip -d /home/aistudio/data/Test/Dataset_2/Feature_File/\n"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# !mkdir -p /home/aistudio/data/Test/Dataset_1/\n",
    "# !unzip /home/aistudio/data/data281827/test_dataset1_velocity.zip -d /home/aistudio/data/Test/Dataset_1/\n",
    "\n",
    "!mv /home/aistudio/data/Test/Dataset_1/* /home/aistudio/data/Test/Dataset_1/Feature_File\n"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import sys\n",
    "sys.path.append(\"./PaddleScience/\")\n",
    "sys.path.append('/home/aistudio/3rd_lib')\n",
    "sys.path.append(\"./model\")\n",
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from timeit import default_timer\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import paddle\n",
    "import yaml\n",
    "from paddle.optimizer.lr import LRScheduler\n",
    "from src.data import instantiate_datamodule\n",
    "from src.networks import instantiate_network\n",
    "from src.utils.average_meter import AverageMeter\n",
    "from src.utils.dot_dict import DotDict\n",
    "from src.utils.dot_dict import flatten_dict\n",
    "\n",
    "class StepDecay(LRScheduler):\n",
    "    def __init__(\n",
    "        self, learning_rate, step_size, gamma=0.1, last_epoch=-1, verbose=False\n",
    "    ):\n",
    "        if not isinstance(step_size, int):\n",
    "            raise TypeError(\n",
    "                \"The type of 'step_size' must be 'int', but received %s.\"\n",
    "                % type(step_size)\n",
    "            )\n",
    "        if gamma >= 1.0:\n",
    "            raise ValueError(\"gamma should be < 1.0.\")\n",
    "\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        super().__init__(learning_rate, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        i = self.last_epoch // self.step_size\n",
    "        return self.base_lr * (self.gamma**i)\n",
    "\n",
    "\n",
    "def instantiate_scheduler(config):\n",
    "    if config.opt_scheduler == \"CosineAnnealingLR\":\n",
    "        scheduler = paddle.optimizer.lr.CosineAnnealingDecay(\n",
    "            config.lr, T_max=config.opt_scheduler_T_max\n",
    "        )\n",
    "    elif config.opt_scheduler == \"StepLR\":\n",
    "        scheduler = StepDecay(\n",
    "            config.lr, step_size=config.opt_step_size, gamma=config.opt_gamma\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Got {config.opt_scheduler=}\")\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "# loss function with rel/abs Lp loss\n",
    "class LpLoss(object):\n",
    "    def __init__(self, d=2, p=2, size_average=True, reduction=True):\n",
    "        super(LpLoss, self).__init__()\n",
    "        # Dimension and Lp-norm type are postive\n",
    "        assert d > 0 and p > 0\n",
    "\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.reduction = reduction\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def abs(self, x, y):\n",
    "        num_examples = x.size()[0]\n",
    "\n",
    "        # Assume uniform mesh\n",
    "        h = 1.0 / (x.size()[1] - 1.0)\n",
    "\n",
    "        all_norms = (h ** (self.d / self.p)) * paddle.norm(\n",
    "            x.reshape((num_examples, -1)) - y.reshape((num_examples, -1)), self.p, 1\n",
    "        )\n",
    "\n",
    "        if self.reduction:\n",
    "            if self.size_average:\n",
    "                return paddle.mean(all_norms)\n",
    "            else:\n",
    "                return paddle.sum(all_norms)\n",
    "\n",
    "        return all_norms\n",
    "\n",
    "    def rel(self, x, y):\n",
    "        diff_norms = paddle.norm(x-y, 2)\n",
    "        y_norms = paddle.norm(y, self.p)\n",
    "\n",
    "        if self.reduction:\n",
    "            if self.size_average:\n",
    "                return paddle.mean(diff_norms / y_norms)\n",
    "            else:\n",
    "                return paddle.sum(diff_norms / y_norms)\n",
    "\n",
    "        return diff_norms / y_norms\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        return self.rel(x, y)\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 0):\n",
    "    paddle.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def str2intlist(s: str) -> List[int]:\n",
    "    return [int(item.strip()) for item in s.split(\",\")]\n",
    "\n",
    "\n",
    "def parse_args(yaml=\"UnetShapeNetCar.yaml\"):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        type=str,\n",
    "        default=\"configs/\"+ yaml,\n",
    "        help=\"Path to the configuration file\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\",\n",
    "        type=str,\n",
    "        default=\"cuda\",\n",
    "        help=\"Device to use for training (cuda or cpu)\",\n",
    "    )\n",
    "    parser.add_argument(\"--lr\", type=float, default=None, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=None, help=\"Batch size\")\n",
    "    parser.add_argument(\"--num_epochs\", type=int, default=None, help=\"Number of epochs\")\n",
    "    parser.add_argument(\n",
    "        \"--checkpoint\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Path to the checkpoint file to resume training\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\",\n",
    "        type=str,\n",
    "        default=\"./output\",\n",
    "        help=\"Path to the output directory\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log\",\n",
    "        type=str,\n",
    "        default=\"log\",\n",
    "        help=\"Path to the log directory\",\n",
    "    )\n",
    "    parser.add_argument(\"--logger_types\", type=str, nargs=\"+\", default=None)\n",
    "    parser.add_argument(\"--seed\", type=int, default=0, help=\"Random seed for training\")\n",
    "    parser.add_argument(\"--model\", type=str, default=None, help=\"Model name\")\n",
    "    parser.add_argument(\n",
    "        \"--sdf_spatial_resolution\",\n",
    "        type=str2intlist,\n",
    "        default=None,\n",
    "        help=\"SDF spatial resolution. Use comma to separate the values e.g. 32,32,32.\",\n",
    "    )\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def load_config(config_path):\n",
    "    def include_constructor(loader, node):\n",
    "        # Get the path of the current YAML file\n",
    "        current_file_path = loader.name\n",
    "\n",
    "        # Get the folder containing the current YAML file\n",
    "        base_folder = os.path.dirname(current_file_path)\n",
    "\n",
    "        # Get the included file path, relative to the current file\n",
    "        included_file = os.path.join(base_folder, loader.construct_scalar(node))\n",
    "\n",
    "        # Read and parse the included file\n",
    "        with open(included_file, \"r\") as file:\n",
    "            return yaml.load(file, Loader=yaml.Loader)\n",
    "\n",
    "    # Register the custom constructor for !include\n",
    "    yaml.Loader.add_constructor(\"!include\", include_constructor)\n",
    "\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "    # Convert to dot dict\n",
    "    config_flat = flatten_dict(config)\n",
    "    config_flat = DotDict(config_flat)\n",
    "    return config_flat\n",
    "\n",
    "import re\n",
    " \n",
    "def extract_numbers(s):\n",
    "    return [int(digit) for digit in re.findall(r'\\d+', s)]\n",
    "\n",
    "\n",
    "def write_to_vtk(out_dict, point_data_pos=\"press on mesh points\", mesh_path=None, track=None):\n",
    "    import meshio\n",
    "    p = out_dict[\"pressure\"]\n",
    "    index = extract_numbers(mesh_path.name)[0]\n",
    "\n",
    "    if track == \"Dataset_1\":\n",
    "        index = str(index).zfill(3)   \n",
    "    elif track == \"Track_B\":\n",
    "        index = str(index).zfill(4)\n",
    "\n",
    "    print(f\"Pressure shape for mesh {index} = {p.shape}\")\n",
    "\n",
    "        \n",
    "    if point_data_pos == \"press on mesh points\":\n",
    "        mesh = meshio.read(mesh_path)\n",
    "        mesh.point_data[\"p\"] = p.numpy()\n",
    "        if \"pred wss_x\" in out_dict:\n",
    "            wss_x = out_dict[\"pred wss_x\"]\n",
    "            mesh.point_data[\"wss_x\"] = wss_x.numpy()\n",
    "    elif point_data_pos == \"press on mesh cells\":\n",
    "        points = np.load(mesh_path.parent / f\"centroid_{index}.npy\")\n",
    "        npoint = points.shape[0]\n",
    "        mesh = meshio.Mesh(\n",
    "            points=points, cells=[(\"vertex\", np.arange(npoint).reshape(npoint, 1))]\n",
    "        )\n",
    "        mesh.point_data = {\"p\":p.numpy()}\n",
    "\n",
    "    print(f\"write : ./output/{mesh_path.parent.name}_{index}.vtk\")\n",
    "    mesh.write(f\"./output/{mesh_path.parent.name}_{index}.vtk\") \n",
    " "
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@paddle.no_grad()\n",
    "def eval(model, datamodule, config, loss_fn=None, track=\"Dataset_1\"):\n",
    "    test_loader = datamodule.test_dataloader(batch_size=config.eval_batch_size, shuffle=False, num_workers=0)\n",
    "    data_list = []\n",
    "    cd_list = []\n",
    "\n",
    "    for i, data_dict in enumerate(test_loader):\n",
    "        out_dict = model.eval_dict(data_dict, loss_fn=loss_fn, decode_fn=datamodule.decode)\n",
    "        if'l2 eval loss' in out_dict: \n",
    "            if i == 0:\n",
    "                data_list.append(['id', 'l2 p'])\n",
    "            else:\n",
    "                data_list.append([i, float(out_dict['l2 eval loss'])])\n",
    "        \n",
    "        # TODO : you may write velocity into vtk, and analysis in your report\n",
    "        if config.write_to_vtk is True:\n",
    "            print(\"datamodule.test_mesh_paths = \", datamodule.test_mesh_paths[i])\n",
    "            write_to_vtk(out_dict, config.point_data_pos, datamodule.test_mesh_paths[i], track)\n",
    "        \n",
    "        # Your submit your npy to leaderboard here\n",
    "        if \"pressure\" in out_dict:\n",
    "            p = out_dict[\"pressure\"].reshape((-1,)).astype(np.float32)\n",
    "            test_indice = datamodule.test_indices[i]\n",
    "            npy_leaderboard = f\"./output/{track}/press_{str(test_indice).zfill(3)}.npy\"\n",
    "            print(f\"saving *.npy file for [{track}] leaderboard : \", npy_leaderboard)\n",
    "            np.save(npy_leaderboard, p)\n",
    "        if \"velocity\" in out_dict:\n",
    "            v = out_dict[\"velocity\"].reshape((-1,3)).astype(np.float32)\n",
    "            test_indice = datamodule.test_indices[i]\n",
    "            npy_leaderboard = f\"./output/{track}/vel_{str(test_indice).zfill(3)}.npy\"\n",
    "            print(f\"saving *.npy file for [{track}] leaderboard : \", npy_leaderboard)\n",
    "            np.save(npy_leaderboard, v)\n",
    "        if \"cd\" in out_dict:\n",
    "            v = out_dict[\"cd\"].item()\n",
    "            test_indice = datamodule.test_indices[i]\n",
    "            cd_list.append([i, v])\n",
    "\n",
    "        # check csv in ./output\n",
    "        with open(f\"./output/{config.project_name}.csv\", \"w\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(data_list)\n",
    "\n",
    "    if \"cd\" in out_dict:\n",
    "        titles = [\"\", \"Cd\"]\n",
    "        df = pd.DataFrame(cd_list, columns=titles)\n",
    "        df.to_csv(f'./output/{track}/Answer.csv', index=False)\n",
    "    return\n",
    "\n",
    "def train(config):\n",
    "    model = instantiate_network(config)\n",
    "    datamodule = instantiate_datamodule(config)\n",
    "    train_loader = datamodule.train_dataloader(batch_size=config.batch_size, shuffle=False)\n",
    "    eval_dict = None\n",
    "    # Initialize the optimizer\n",
    "    scheduler = instantiate_scheduler(config)\n",
    "    optimizer = paddle.optimizer.Adam(\n",
    "        parameters=model.parameters(), learning_rate=scheduler, weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    # Initialize the loss function\n",
    "    loss_fn = LpLoss(size_average=True)\n",
    "    L2 = []\n",
    "    for ep in range(config.num_epochs):\n",
    "        model.train()\n",
    "        t1 = default_timer()\n",
    "        train_l2_meter = AverageMeter()\n",
    "        # train_reg = 0\n",
    "        for i, data_dict in enumerate(train_loader):\n",
    "            optimizer.clear_grad()\n",
    "            loss_dict = model.loss_dict(data_dict, loss_fn=loss_fn)\n",
    "            loss = 0\n",
    "            for k, v in loss_dict.items():\n",
    "                loss = loss + v.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_l2_meter.update(loss.item())\n",
    "        scheduler.step()\n",
    "        t2 = default_timer()\n",
    "        print(\n",
    "            f\"Training epoch {ep} took {t2 - t1:.2f} seconds. L2 loss: {train_l2_meter.avg:.4f}\"\n",
    "        )\n",
    "\n",
    "        L2.append(train_l2_meter.avg)\n",
    "        if ep % config.eval_interval == 0 or ep == config.num_epochs - 1:\n",
    "            # if you want to eval in Cars during training process, use data in Training datasets\n",
    "            # eval_dict = eval(model, datamodule, config, loss_fn)\n",
    "            if eval_dict is not None:\n",
    "                for k, v in eval_dict.items():\n",
    "                    print(f\"Epoch: {ep} {k}: {v.item():.4f}\")\n",
    "        # Save the weights\n",
    "        if ep % config.save_interval == 0 or ep == config.num_epochs - 1 and ep > 1:\n",
    "            paddle.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(\"./output/\", f\"model-{config.model}-{config.track}-{ep}.pdparams\"),\n",
    "            )\n",
    "\n",
    "def load_yaml(file_name):\n",
    "    args = parse_args(file_name)\n",
    "    # args = parse_args(\"Unet_Velocity.yaml\")\n",
    "    config = load_config(args.config)\n",
    "\n",
    "    # Update config with command line arguments\n",
    "    for key, value in vars(args).items():\n",
    "        if key != \"config\" and value is not None:\n",
    "            config[key] = value\n",
    "\n",
    "    # pretty print the config\n",
    "    if paddle.distributed.get_rank() == 0:\n",
    "        print(f\"\\n--------------- Config [{file_name}] Table----------------\")\n",
    "        for key, value in config.items():\n",
    "            print(\"Key: {:<30} Val: {}\".format(key, value))\n",
    "        print(\"--------------- Config yaml Table----------------\\n\")\n",
    "    return config\n",
    "\n",
    "def leader_board(config, track):\n",
    "    os.makedirs(f\"./output/{track}/\", exist_ok=True)\n",
    "    model = instantiate_network(config)\n",
    "    checkpoint = paddle.load(f\"./output/model-{config.model}-{config.track}-{config.num_epochs - 1}.pdparams\")\n",
    "    model.load_dict(checkpoint)\n",
    "    print(f\"\\n-------Starting Evaluation over [{config.track}] --------\")\n",
    "    config.n_train = 1\n",
    "    t1 = default_timer()\n",
    "    \n",
    "    config.mode=\"test\"\n",
    "    eval_dict = eval(\n",
    "        model, instantiate_datamodule(config), config, loss_fn=lambda x,y:0, track=track\n",
    "    )\n",
    "    t2 = default_timer()\n",
    "    print(f\"Inference over [Dataset_1 pressure] took {t2 - t1:.2f} seconds.\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"./output/\", exist_ok=True)\n",
    "    config_p = load_yaml(\"UnetShapeNetCar.yaml\")\n",
    "    config_p.n_test=50\n",
    "    train(config_p)\n",
    "    \n",
    "    index_list = np.loadtxt(\"/home/aistudio/data/train_data_1_velocity/watertight_meshes.txt\", dtype=int)\n",
    "    config_v = load_yaml(\"Unet_Velocity.yaml\")\n",
    "    config_v.train_index_list = index_list[:100].tolist()  \n",
    "    config_v.test_index_list = index_list[500:550].tolist()  \n",
    "    train(config_v)\n",
    "\n",
    "    config_cd = load_yaml(\"Unet_Cd.yaml\")\n",
    "    index_list = np.loadtxt(\"/home/aistudio/data/Training/Dataset_2/Label_File/dataset2_train_label.csv\", delimiter=\",\", dtype=str, encoding='utf-8')[:,1][1:]\n",
    "    config_cd.train_index_list = index_list[:100].tolist()  \n",
    "    config_cd.test_index_list = index_list[500:550].tolist()  \n",
    "    train(config_cd)\n",
    "\n",
    "    # test on leader_board, or do evaluation by yourself\n",
    "    leader_board(config_cd, \"Gen_Answer\")\n",
    "    leader_board(config_p,  \"Gen_Answer\")\n",
    "    leader_board(config_v,  \"Gen_Answer\")\n",
    "    os.system(f\"zip -r -j ./output/Gen_Answer.zip ./output/Gen_Answer\")\n"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "09f0dbf7b1569c1ab842ae2f41770fe6aa1b54326d081112fa5944b99abb5899"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
